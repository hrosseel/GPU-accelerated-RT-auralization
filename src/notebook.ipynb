{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch, os, math\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchvision import io\n",
    "\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from types import SimpleNamespace as ns\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(output, filter_td):\n",
    "    for filter_idx, (filter_part, signal_part) in enumerate(zip(filter_td, signal)):\n",
    "        output_ref = np.convolve(signal_part, filter_part)\n",
    "        print(\"Results are equal:\", ((output[filter_idx, :] - output_ref[:output.shape[1]]) ** 2 < 1e-8).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10 # Number of filter channels\n",
    "C_in = 1 # Number of input channels\n",
    "B = 256  # Block length\n",
    "FL = 15 * B  # Filter length\n",
    "K = FL // B  # Number of partitions\n",
    "I = 1000  # Number of input blocks\n",
    "\n",
    "output = []\n",
    "\n",
    "# Define the signal batch\n",
    "signal = np.random.randn(C_in, B * I)\n",
    "signal_batch = signal.reshape(C_in, I, B).swapaxes(0, 1)\n",
    "\n",
    "# Define the frequency-domain delay line (FDL)\n",
    "fdl = np.zeros((C_in, B + 1, K), dtype=np.complex64)\n",
    "fdl_cursor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filter as a random array\n",
    "filter_td = np.random.randn(C, FL).astype(np.float64)\n",
    "\n",
    "# Partition the filter into blocks of length B\n",
    "filter_parts = np.pad(filter_td, ((0, 0), (0, K * B - FL)), mode='constant').reshape(C, B, K, order='F')\n",
    "\n",
    "assert (filter_parts[0, :B, 0] == filter_td[0, :B]).all()\n",
    "\n",
    "# Partition the filter into blocks of length B, and zero-pad another B samples\n",
    "filters_padded = np.pad(filter_parts, ((0, 0), (0, B), (0, 0)), mode='constant')  # shape: (C, 2 * B, K)\n",
    "\n",
    "# Compute the RFFT of the filters (real-to-complex FFT)\n",
    "filters_fd = np.fft.rfft(filters_padded, axis=1)  # shape: (C, B + 1, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================# %% Simulate input streaming by looping over the batches\n",
    "output = []\n",
    "input_buffer_td = np.zeros((C_in, 2 * B))\n",
    "for signal_td in signal_batch:\n",
    "\n",
    "    # 2. Packing and RFFT of the signal (online)\n",
    "    # ============================================================================\n",
    "    # # Partition the signal into blocks of length B, and zero-pad another B samples\n",
    "    # signal_td_padded = np.pad(signal_td, (0, B), mode='constant')  # shape: (2 * B,)\n",
    "\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:, :B] = input_buffer_td[:, B:]\n",
    "    input_buffer_td[:, B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td, axis=1)  # shape: (C, B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # ============================================================================\n",
    "    # Store the fd signal in a frequency-domain delay line\n",
    "    fdl[:, :, fdl_cursor] = input_fd\n",
    "    \n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd = np.zeros((C, B + 1), dtype=np.complex64)\n",
    "    for k in range(K):\n",
    "        cursor = (fdl_cursor - k + K) % K\n",
    "        output_fd += fdl[:, :, cursor] * filters_fd[:, :, k]\n",
    "\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd, axis=1)  # shape: (C, 2 * B,)\n",
    "    # Only keep the first B samples\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(output, filter_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Simulation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate kernel execution\n",
    "def blk_kernel(f, blocks, threads, *args):\n",
    "    for i in range(blocks):\n",
    "        for j in range(threads): f(i, j, threads, *args)\n",
    "\n",
    "def part_conv_gpu(input_fd, fdl_cursor):\n",
    "    # define the output spectrum\n",
    "    output_fd = torch.empty((C * (B + 1)), dtype=torch.complex64)\n",
    "\n",
    "    threads = 256\n",
    "    blocks = math.ceil(C * (B + 1) / threads)\n",
    "    \n",
    "    # Store the fd signal in a frequency-domain delay line\n",
    "    fdl[:, :, fdl_cursor] = input_fd\n",
    "    blk_kernel(conv_kernel, blocks, threads, fdl.ravel(), filters_fd.ravel(), fdl_cursor, output_fd, K, B, C)\n",
    "\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "\n",
    "    return output_fd.reshape(C, B + 1), fdl_cursor\n",
    "\n",
    "def conv_kernel(blockidx, threadidx, blockdim, fdl, filters_fd,\n",
    "                fdl_cursor, output_fd, K, B, C):\n",
    "    thread_id = blockidx * blockdim + threadidx\n",
    "    \n",
    "    if thread_id >= C * (B + 1):\n",
    "        return\n",
    "\n",
    "    channel_id = thread_id // (B + 1)\n",
    "    bin_id = thread_id % (B + 1)\n",
    "    cursor = fdl_cursor\n",
    "\n",
    "    fdl_offset = bin_id * K\n",
    "    filter_offset = channel_id * ((B + 1) * K) + bin_id * K\n",
    "    output_offset = channel_id * (B + 1) + bin_id\n",
    "\n",
    "    out = 0\n",
    "    for k in range(K):\n",
    "        out += fdl[fdl_offset + cursor] * filters_fd[filter_offset + k]\n",
    "        cursor = (cursor - 1 + K) % K\n",
    "    \n",
    "    output_fd[output_offset] = out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# %% Simulate input streaming by looping over the batches\n",
    "fdl = np.zeros((C_in, B + 1, K), dtype=np.complex64)\n",
    "fdl_cursor = 0\n",
    "\n",
    "input_buffer_td = np.zeros((C_in, 2 * B))\n",
    "output = []\n",
    "\n",
    "for signal_td in signal_batch:\n",
    "\n",
    "    # 2. Packing and RFFT of the signal (online)\n",
    "    # ============================================================================\n",
    "    # # Partition the signal into blocks of length B, and zero-pad another B samples\n",
    "    # signal_td_padded = np.pad(signal_td, (0, B), mode='constant')  # shape: (2 * B,)\n",
    "\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:, :B] = input_buffer_td[:, B:]\n",
    "    input_buffer_td[:, B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td, axis=1)  # shape: (B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # ============================================================================\n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd, fdl_cursor = part_conv_gpu(input_fd, fdl_cursor)\n",
    "    assert output_fd.isnan().any() == False\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd, axis=1)  # shape: (C, 2 * B)\n",
    "    # Only keep the first B samples and append to the output\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(output, filter_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU implementation in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_begin = r'''\n",
    "using namespace torch::indexing;\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "\n",
    "__global__ void conv_kernel(const c10::complex<float>* fdl, const c10::complex<float>* filters_fd, int fdl_cursor, c10::complex<float>* output_fd) {\n",
    "    \n",
    "    const int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (thread_id >= NUM_CHANNELS * (BLOCK_SIZE + 1)) return;\n",
    "\n",
    "    const int channel_id = thread_id / (BLOCK_SIZE + 1);\n",
    "    const int bin_id = thread_id % (BLOCK_SIZE + 1);\n",
    "    int cursor = fdl_cursor;\n",
    "\n",
    "    const int fdl_offset = bin_id * NUM_PARTS;\n",
    "    const int filter_offset = channel_id * ((BLOCK_SIZE + 1) * NUM_PARTS) + bin_id * NUM_PARTS;\n",
    "    const int output_offset = channel_id * (BLOCK_SIZE + 1) + bin_id;\n",
    "\n",
    "    c10::complex<float> out = 0;\n",
    "    for (int k = 0; k < NUM_PARTS; ++k) {\n",
    "        out += fdl[fdl_offset + cursor] * filters_fd[filter_offset + k];\n",
    "        cursor = (cursor - 1 + NUM_PARTS) % NUM_PARTS;\n",
    "    }\n",
    "    output_fd[output_offset] = out;\n",
    "}\n",
    "\n",
    "__global__ void conv_kernel_multi(const c10::complex<float>* fdl, const c10::complex<float>* filters_fd, int fdl_cursor, c10::complex<float>* output_fd) {\n",
    "    \n",
    "    const int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (thread_id >= NUM_CHANNELS * (BLOCK_SIZE + 1)) return;\n",
    "\n",
    "    const int channel_id = thread_id / (BLOCK_SIZE + 1);\n",
    "    const int bin_id = thread_id % (BLOCK_SIZE + 1);\n",
    "    int cursor = fdl_cursor;\n",
    "\n",
    "    const int filter_offset = channel_id * ((BLOCK_SIZE + 1) * NUM_PARTS) + bin_id * NUM_PARTS;\n",
    "    const int output_offset = channel_id * (BLOCK_SIZE + 1) + bin_id;\n",
    "\n",
    "    c10::complex<float> out = 0;\n",
    "    for (int k = 0; k < NUM_PARTS; ++k) {\n",
    "        out += fdl[filter_offset + cursor] * filters_fd[filter_offset + k];\n",
    "        cursor = (cursor - 1 + NUM_PARTS) % NUM_PARTS;\n",
    "    }\n",
    "    output_fd[output_offset] = out;\n",
    "}\n",
    "\n",
    "torch::Tensor part_conv_gpu(torch::Tensor input_fd, torch::Tensor fdl, torch::Tensor filters_fd, int fdl_cursor) {\n",
    "    CHECK_INPUT(input_fd);\n",
    "    CHECK_INPUT(fdl);\n",
    "    CHECK_INPUT(filters_fd);\n",
    "\n",
    "    auto output_fd = torch::empty({NUM_CHANNELS, BLOCK_SIZE+1}, input_fd.options());\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = cdiv(NUM_CHANNELS * (BLOCK_SIZE + 1), threads);\n",
    "\n",
    "    if (fdl.dim() == 2) {\n",
    "        // Store the fd signal in a frequency-domain delay line\n",
    "        fdl.index_put_({Slice(0, BLOCK_SIZE + 1), fdl_cursor}, input_fd);\n",
    "        conv_kernel<<<blocks, threads>>>(fdl.data_ptr<c10::complex<float>>(), filters_fd.data_ptr<c10::complex<float>>(), fdl_cursor, output_fd.data_ptr<c10::complex<float>>());\n",
    "\n",
    "    } else if (fdl.dim() == 3) {\n",
    "        // Store the fd signal in a frequency-domain delay line\n",
    "        fdl.index_put_({Slice(), Slice(0, BLOCK_SIZE + 1), fdl_cursor}, input_fd);\n",
    "        conv_kernel_multi<<<blocks, threads>>>(fdl.data_ptr<c10::complex<float>>(), filters_fd.data_ptr<c10::complex<float>>(), fdl_cursor, output_fd.data_ptr<c10::complex<float>>());\n",
    "\n",
    "    } else {\n",
    "        throw std::runtime_error(\"fdl must be 2D or 3D\");\n",
    "    }\n",
    "\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK(); // Check for errors\n",
    "    return output_fd;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cuda(cuda_src, cpp_src, funcs, K, B, C, verbose=False):\n",
    "    return load_inline(\n",
    "        cuda_sources=[cuda_src],\n",
    "        cpp_sources=[cpp_src],\n",
    "        functions=funcs,\n",
    "        extra_cuda_cflags=[\"-O2\", f\"-DNUM_CHANNELS={C}\", f\"-DBLOCK_SIZE={B}\", f\"-DNUM_PARTS={K}\"],\n",
    "        verbose=verbose,\n",
    "        name=\"inline_ext\"\n",
    "    )\n",
    "\n",
    "cpp_src = \"torch::Tensor part_conv_gpu(torch::Tensor input_fd, torch::Tensor fdl, torch::Tensor filters_fd, int fdl_cursor);\"\n",
    "module = load_cuda(cuda_src, cpp_src, ['part_conv_gpu'], K, B, C, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Reset parameters\n",
    "filters_fd_gpu = torch.tensor(filters_fd, dtype=torch.complex64, device='cuda').contiguous()\n",
    "fdl_gpu = torch.zeros((C, B + 1, K), dtype=torch.complex64, device='cuda').contiguous()\n",
    "\n",
    "fdl_cursor = 0\n",
    "input_buffer_td = np.zeros((C, 2 * B))\n",
    "output = []\n",
    "\n",
    "# Simulate input streaming by looping over the batches\n",
    "for signal_td in signal_batch:\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:, :B] = input_buffer_td[:, B:]\n",
    "    input_buffer_td[:, B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td)  # shape: (B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # =================================================================\n",
    "    \n",
    "    # Move the input to the GPU\n",
    "    input_fd_gpu = torch.tensor(input_fd, dtype=torch.complex64, device='cuda').contiguous()\n",
    "\n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd = module.part_conv_gpu(input_fd_gpu, fdl_gpu, filters_fd_gpu, fdl_cursor)\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd.cpu(), axis=1)  # shape: (C, 2 * B)\n",
    "    # Only keep the first B samples and append to the output\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "check_output(output, filter_td)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
