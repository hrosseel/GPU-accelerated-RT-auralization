{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch, os, math\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchvision import io\n",
    "\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from types import SimpleNamespace as ns\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(output, filter_td):\n",
    "    for filter_idx, filter_part in enumerate(filter_td):\n",
    "        output_ref = np.convolve(signal, filter_part)\n",
    "        print(\"Results are equal:\", ((output[filter_idx, :-1] - output_ref) ** 2 < 1e-8).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10 # Number of channels\n",
    "B = 256  # Block length\n",
    "FL = 10 * B  # Filter length\n",
    "K = FL // B  # Number of partitions\n",
    "I = 1000  # Number of input blocks\n",
    "\n",
    "output = []\n",
    "\n",
    "# Simulate input streaming by dividing the signal into batches of length B\n",
    "# signal = np.random.randn(I * B).astype(np.float64)\n",
    "signal = np.zeros(I * B).astype(np.float64)\n",
    "signal[0] = 1\n",
    "signal_batch = signal.reshape(I, B)\n",
    "# Add batches of zeros to ensure shape is (4 + 16, B)\n",
    "signal_batch = np.append(signal_batch, np.zeros((K, B)), axis=0)\n",
    "\n",
    "# Define the frequency-domain delay line (FDL)\n",
    "fdl = np.zeros((B + 1, K), dtype=np.complex64)\n",
    "fdl_cursor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filter as a random array\n",
    "filter_td = np.random.randn(C, FL).astype(np.float64)\n",
    "\n",
    "# Partition the filter into blocks of length B\n",
    "filter_parts = np.pad(filter_td, ((0, 0), (0, K * B - FL)), mode='constant').reshape(C, B, K, order='F')\n",
    "\n",
    "assert (filter_parts[0, :B, 0] == filter_td[0, :B]).all()\n",
    "\n",
    "# Partition the filter into blocks of length B, and zero-pad another B samples\n",
    "filters_padded = np.pad(filter_parts, ((0, 0), (0, B), (0, 0)), mode='constant')  # shape: (C, 2 * B, K)\n",
    "\n",
    "# Compute the RFFT of the filters (real-to-complex FFT)\n",
    "filters_fd = np.fft.rfft(filters_padded, axis=1)  # shape: (C, B + 1, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================# %% Simulate input streaming by looping over the batches\n",
    "output = []\n",
    "input_buffer_td = np.zeros(2 * B)\n",
    "for signal_td in signal_batch:\n",
    "\n",
    "    # 2. Packing and RFFT of the signal (online)\n",
    "    # ============================================================================\n",
    "    # # Partition the signal into blocks of length B, and zero-pad another B samples\n",
    "    # signal_td_padded = np.pad(signal_td, (0, B), mode='constant')  # shape: (2 * B,)\n",
    "\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:B] = input_buffer_td[B:]\n",
    "    input_buffer_td[B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td)  # shape: (B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # ============================================================================\n",
    "    # Store the fd signal in a frequency-domain delay line\n",
    "    fdl[:, fdl_cursor] = input_fd\n",
    "    \n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd = np.zeros((C, B+1), dtype=np.complex64)\n",
    "    for k in range(K):\n",
    "        cursor = (fdl_cursor - k + K) % K\n",
    "        output_fd += fdl[np.newaxis, :, cursor] * filters_fd[:, :, k]\n",
    "\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd, axis=1)  # shape: (C, 2 * B,)\n",
    "    # Only keep the first B samples\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "check_output(output, filter_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Simulation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate kernel execution\n",
    "def blk_kernel(f, blocks, threads, *args):\n",
    "    for i in range(blocks):\n",
    "        for j in range(threads): f(i, j, threads, *args)\n",
    "\n",
    "def part_conv_gpu(input_fd, fdl_cursor):\n",
    "    # define the output spectrum\n",
    "    output_fd = torch.empty((C * (B + 1)), dtype=torch.complex64)\n",
    "\n",
    "    threads = 256\n",
    "    blocks = math.ceil(C * (B + 1) / threads)\n",
    "    \n",
    "    # Store the fd signal in a frequency-domain delay line\n",
    "    fdl[:, fdl_cursor] = input_fd\n",
    "    blk_kernel(conv_kernel, blocks, threads, fdl.ravel(), filters_fd.ravel(), fdl_cursor, output_fd,\n",
    "               K, B, C)\n",
    "\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "\n",
    "    return output_fd.reshape(C, B + 1), fdl_cursor\n",
    "\n",
    "def conv_kernel(blockidx, threadidx, blockdim, fdl, filters_fd,\n",
    "                fdl_cursor, output_fd, K, B, C):\n",
    "    thread_id = blockidx * blockdim + threadidx\n",
    "    \n",
    "    if thread_id >= C * (B + 1):\n",
    "        return\n",
    "\n",
    "    channel_id = thread_id // (B + 1)\n",
    "    bin_id = thread_id % (B + 1)\n",
    "    cursor = fdl_cursor\n",
    "\n",
    "    fdl_offset = bin_id * K\n",
    "    filter_offset = channel_id * ((B + 1) * K) + bin_id * K\n",
    "    output_offset = channel_id * (B + 1) + bin_id\n",
    "\n",
    "    out = 0\n",
    "    for k in range(K):\n",
    "        out += fdl[fdl_offset + cursor] * filters_fd[filter_offset + k]\n",
    "        cursor = (cursor - 1 + K) % K\n",
    "    \n",
    "    output_fd[output_offset] = out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# %% Simulate input streaming by looping over the batches\n",
    "fdl = np.zeros((B + 1, K), dtype=np.complex64)\n",
    "fdl_cursor = 0\n",
    "\n",
    "input_buffer_td = np.zeros(2 * B)\n",
    "output = []\n",
    "\n",
    "for signal_td in signal_batch:\n",
    "\n",
    "    # 2. Packing and RFFT of the signal (online)\n",
    "    # ============================================================================\n",
    "    # # Partition the signal into blocks of length B, and zero-pad another B samples\n",
    "    # signal_td_padded = np.pad(signal_td, (0, B), mode='constant')  # shape: (2 * B,)\n",
    "\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:B] = input_buffer_td[B:]\n",
    "    input_buffer_td[B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td)  # shape: (B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # ============================================================================\n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd, fdl_cursor = part_conv_gpu(input_fd, fdl_cursor)\n",
    "    assert output_fd.isnan().any() == False\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd, axis=1)  # shape: (C, 2 * B)\n",
    "    # Only keep the first B samples and append to the output\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "check_output(output, filter_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU implementation in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n",
    "    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n",
    "                       extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_begin = r'''\n",
    "using namespace torch::indexing;\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void conv_kernel(const c10::complex<double>* fdl, const c10::complex<double>* filters_fd, int fdl_cursor, c10::complex<double>* output_fd, int K, int B, int C) {\n",
    "    \n",
    "    const int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (thread_id >= C * (B + 1)) return;\n",
    "\n",
    "    const int channel_id = thread_id / (B + 1);\n",
    "    const int bin_id = thread_id % (B + 1);\n",
    "    int cursor = fdl_cursor;\n",
    "\n",
    "    const int fdl_offset = bin_id * K;\n",
    "    const int filter_offset = channel_id * ((B + 1) * K) + bin_id * K;\n",
    "    const int output_offset = channel_id * (B + 1) + bin_id;\n",
    "\n",
    "    c10::complex<double> out = 0;\n",
    "    for (int k = 0; k < K; ++k) {\n",
    "        out += fdl[fdl_offset + cursor] * filters_fd[filter_offset + k];\n",
    "        cursor = (cursor - 1 + K) % K;\n",
    "    }\n",
    "    output_fd[output_offset] = out;\n",
    "}\n",
    "\n",
    "torch::Tensor part_conv_gpu(torch::Tensor input_fd, torch::Tensor fdl, torch::Tensor filters_fd, int fdl_cursor, int K, int B, int C) {\n",
    "    CHECK_INPUT(input_fd);\n",
    "    CHECK_INPUT(fdl);\n",
    "    CHECK_INPUT(filters_fd);\n",
    "\n",
    "    auto output_fd = torch::empty({C, B+1}, input_fd.options());\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = cdiv(C * (B + 1), threads);\n",
    "\n",
    "    // Store the fd signal in a frequency-domain delay line\n",
    "    fdl.index_put_({Slice(0, B+1), fdl_cursor}, input_fd);\n",
    "\n",
    "    conv_kernel<<<blocks, threads>>>(fdl.data_ptr<c10::complex<double>>(), filters_fd.data_ptr<c10::complex<double>>(), fdl_cursor, output_fd.data_ptr<c10::complex<double>>(), K, B, C);\n",
    "    \n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output_fd;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/dspuser/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/dspuser/.cache/torch_extensions/py311_cu124/inline_ext/build.ninja...\n",
      "/home/dspuser/Documents/hrosseel/python/.venv/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module inline_ext...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module inline_ext...\n"
     ]
    }
   ],
   "source": [
    "cpp_src = \"torch::Tensor part_conv_gpu(torch::Tensor input_fd, torch::Tensor fdl, torch::Tensor filters_fd, int fdl_cursor, int K, int B, int C);\"\n",
    "\n",
    "module = load_cuda(cuda_src, cpp_src, ['part_conv_gpu'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the filters to the GPU\n",
    "filters_fd_gpu = torch.tensor(filters_fd, dtype=torch.complex128, device='cuda').contiguous()\n",
    "fdl_gpu = torch.zeros((B + 1, K), dtype=torch.complex128, device='cuda').contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Reset parameters\n",
    "fdl_cursor = 0\n",
    "input_buffer_td = np.zeros(2 * B)\n",
    "output = []\n",
    "\n",
    "# Simulate input streaming by looping over the batches\n",
    "for signal_td in signal_batch:\n",
    "    # Put the incoming signal in the input buffer after sliding the previous signal\n",
    "    input_buffer_td[:B] = input_buffer_td[B:]\n",
    "    input_buffer_td[B:] = signal_td\n",
    "\n",
    "    # Compute the RFFT of the signals (real-to-complex FFT)\n",
    "    input_fd = np.fft.rfft(input_buffer_td)  # shape: (B + 1)\n",
    "\n",
    "    # The following code is executed on the GPU\n",
    "    # =================================================================\n",
    "    \n",
    "    # Move the input to the GPU\n",
    "    input_fd_gpu = torch.tensor(input_fd, dtype=torch.complex128, device='cuda').contiguous()\n",
    "\n",
    "    # Perform the complex multiplication between the fdl and the filter partitions\n",
    "    output_fd = module.part_conv_gpu(input_fd_gpu, fdl_gpu, filters_fd_gpu, fdl_cursor, K, B, C)\n",
    "    fdl_cursor = (fdl_cursor + 1) % K  # Update the index\n",
    "\n",
    "    # ============================================================================\n",
    "    # The following code is executed on the CPU\n",
    "\n",
    "    # Perform the inverse RFFT to obtain the output signal\n",
    "    output_td = np.fft.irfft(output_fd.cpu(), axis=1)  # shape: (C, 2 * B)\n",
    "    # Only keep the first B samples and append to the output\n",
    "    output.append(output_td[:, B:].T)\n",
    "\n",
    "output = np.array(output).reshape(-1, C).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n",
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "# Check the output\n",
    "check_output(output, filter_td)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
